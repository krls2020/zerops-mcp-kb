{
  "type": "kafka",
  "category": "messaging",
  "description": "Distributed event streaming platform for high-throughput data pipelines",
  "versions": [
    { "version": "3", "status": "current", "recommended": true, "note": "Always uses latest stable" }
  ],
  "modes": ["HA"],
  "ports": {
    "broker": 9092,
    "internal": true,
    "httpSupport": false
  },
  "configuration": {
    "yamlImport": {
      "required": ["hostname", "type"],
      "optional": ["minContainers", "maxContainers", "envVariables"],
      "defaults": {
        "mode": "HA"
      }
    },
    "resources": {
      "minContainers": 3,
      "maxContainers": { "max": 9 },
      "verticalScaling": {
        "cpu": { "min": 2, "max": 32, "units": "cores" },
        "ram": { "min": 2, "max": 128, "units": "GB" },
        "disk": { "min": 10, "max": 5000, "units": "GB" }
      }
    },
    "cluster": {
      "zookeeper": "Integrated KRaft mode (no external Zookeeper)",
      "replication": "Configurable replication factor",
      "partitions": "Configurable partition count"
    }
  },
  "envVariables": {
    "system": {
      "KAFKA_HEAP_OPTS": "-Xmx1G -Xms1G",
      "KAFKA_LOG_DIRS": "/var/lib/kafka"
    },
    "autoGenerated": {
      "hostname": "Internal hostname for connections",
      "bootstrap_servers": "Kafka bootstrap servers list",
      "host": "Kafka broker host",
      "port": "Kafka broker port (9092)"
    }
  },
  "access": {
    "internal": "Accessible from other services via ${servicename_hostname}:9092",
    "external": "No direct external access, use VPN for management",
    "auth": "SASL/PLAIN authentication available"
  },
  "features": [
    "High-throughput message streaming",
    "Distributed commit log",
    "Horizontal scalability",
    "Fault tolerance with replication",
    "Exactly-once semantics",
    "Stream processing",
    "Connect API for integrations",
    "Long-term message retention",
    "Compacted topics",
    "Message ordering guarantees"
  ],
  "concepts": {
    "topics": {
      "description": "Categories for messages",
      "partitions": "Parallel processing units",
      "replication": "Fault tolerance"
    },
    "producers": {
      "description": "Applications that publish messages",
      "acks": ["0 (fire-and-forget)", "1 (leader)", "all (all replicas)"]
    },
    "consumers": {
      "description": "Applications that read messages",
      "groups": "Load balancing and fault tolerance",
      "offsets": "Position tracking"
    }
  },
  "bestPractices": [
    "Use appropriate replication factor (typically 3)",
    "Configure proper retention policies",
    "Monitor disk usage and performance",
    "Use consumer groups for scaling",
    "Set appropriate batch sizes",
    "Configure proper acknowledgment levels",
    "Use schema registry for data contracts",
    "Implement proper error handling"
  ],
  "clientLibraries": {
    "javascript": "kafkajs",
    "python": "confluent-kafka-python",
    "java": "org.apache.kafka:kafka-clients",
    "go": "github.com/segmentio/kafka-go",
    "dotnet": "Confluent.Kafka",
    "rust": "rdkafka"
  },
  "connectionExamples": {
    "nodejs": {
      "code": "import { Kafka } from 'kafkajs';\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['${hostname}:9092']\n});"
    },
    "python": {
      "code": "from confluent_kafka import Producer\nproducer = Producer({\n  'bootstrap.servers': '${hostname}:9092'\n})"
    },
    "java": {
      "code": "Properties props = new Properties();\nprops.put(\"bootstrap.servers\", \"${hostname}:9092\");\nKafkaProducer<String, String> producer = new KafkaProducer<>(props);"
    }
  },
  "useCases": {
    "eventStreaming": {
      "description": "Real-time event processing",
      "examples": ["User activity tracking", "IoT data ingestion", "Log aggregation"]
    },
    "dataIntegration": {
      "description": "Connect systems via streaming",
      "examples": ["Database CDC", "ETL pipelines", "Data synchronization"]
    },
    "messaging": {
      "description": "Async communication between services",
      "patterns": ["Event sourcing", "CQRS", "Saga pattern"]
    },
    "analytics": {
      "description": "Stream processing and analytics",
      "tools": ["Kafka Streams", "ksqlDB", "Apache Flink"]
    }
  },
  "topicConfiguration": {
    "replication": {
      "factor": "Number of replicas",
      "minInSync": "Minimum replicas for writes"
    },
    "retention": {
      "time": "How long to keep messages",
      "size": "Maximum size per partition"
    },
    "compression": {
      "types": ["none", "gzip", "snappy", "lz4", "zstd"]
    }
  },
  "examples": {
    "basic": {
      "yamlImport": {
        "services": [
          {
            "hostname": "kafka",
            "type": "kafka@3",
            "minContainers": 3
          }
        ]
      }
    },
    "withStreamProcessing": {
      "yamlImport": {
        "services": [
          {
            "hostname": "kafka",
            "type": "kafka@3",
            "minContainers": 3,
            "maxContainers": 5
          },
          {
            "hostname": "producer",
            "type": "nodejs@20",
            "envVariables": {
              "KAFKA_BROKERS": "${kafka_hostname}:9092"
            }
          },
          {
            "hostname": "consumer",
            "type": "nodejs@20",
            "envVariables": {
              "KAFKA_BROKERS": "${kafka_hostname}:9092"
            }
          }
        ]
      }
    }
  },
  "performance": {
    "throughput": "Hundreds of thousands of messages/second",
    "latency": "Low millisecond latency",
    "retention": "Terabytes of data retention"
  },
  "monitoring": {
    "metrics": "JMX metrics available",
    "tools": ["Kafka Manager", "Kafdrop", "Confluent Control Center"]
  }
}